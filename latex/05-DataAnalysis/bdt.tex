%!TEX root = ../main.tex

\subsection{Multivariate selection}
\label{sec:dataanalysis:selection:bdt}

More elaborate selection methods are based on machine learning algorithms,
which more and more enter the field of particle physics. These multivariate
techniques improve the possibility to separate signal from background
contributions as they make use of correlations between input variables.
Software packages, like TMVA~\cite{Hocker:2007ht} or
scikit-learn~\cite{scikit-learn}, provide implementations of these algorithms.

A simple multivariate classifier is a decision tree~\cite{Breiman}, which
splits the phase space according to repeated decisions on certain variables.
Starting from a root node the variable and cut value is determined where the
best separation can be achieved according to a criterion like the Gini
index~\cite{giniindex}, $p \cdot (1-p)$ with $p$ being the signal purity. \ie
the fraction of signal in the total data sample. At each following node the
cut value and even the variable used for the separation can be chosen
dependent on the previous decision. The depth of the tree, \ie the maximal
number of consecutive decisions, is tunable. When a stop criterion is matched,
\eg the ratio of candidates reaching a node falls below a predefined
threshold, no further decisions are applied. The tree is trained with a set of
labelled data. Each final leaf is classified as signal or background depending
on the class of the majority of training events ending in that leaf.

An improvement can be achieved by combining several decision trees to a
forest, in which the classification of an event follows a majority vote
of the individual trees, \eg in a Random Forest~\cite{RandomForest}.

An alternative is to alter the impact of the training events in a decision
tree by applying weights. This procedure is called boosting and leads to
Boosted Decision Trees (BDT)~\cite{Roe}. One possibility how the
boosting can be realised is the AdaBoost method~\cite{AdaBoost}. Events that
are misidentified in the previous tree are weighted by
\begin{align}
	\alpha = \frac{1 - \eps}{\eps}\,,
\end{align}
with $\eps < \num{0.5}$ being the misclassification rate of the previous tree.
This criterion is fulfilled as long as the decision tree performs better than
a random choice. The learning rate can be further modified by using an
exponent $\beta$ for the weight, $\alpha \to \alpha^{\beta}$. To have the same
effective number of events as before, \ie the same sum of weights, the events
are renormalised before training the next tree. The BDT output classifier $y$
is given by the weighted mean of the tree's output $h$, which is \num{+1} for
signal and \num{-1} for background,
\begin{align}
	y = \frac{1}{N_{\textrm{trees}}} \sum_i^{N_{\textrm{trees}}} \ln(\alpha_i) h_i\,.
\end{align}
This means that an event is classified more signal-like the higher the BDT
output classifier. Another boosting algorithm is the GradientBoost
method~\cite{GradientBoost}. It is based on minimising a defined loss
function, which describes the deviation between the classification and the
truth, by calculating the gradient of the loss function.

The importance of a feature in a (boosted) decision tree can be determined by
counting how often it is used and by considering how important the cuts are in
terms of events reaching the corresponding node and in terms of the separation
power between signal and background candidates.

Another multivariate method is the Artificial Neural Network
(ANN)~\cite{Denby:1987rk}. It consists of several neurons, divided into input
units, hidden units and output units. The way how these units are connected,
the corresponding strength and the effect, either excitatory or inhibitory,
defines the neural network.
